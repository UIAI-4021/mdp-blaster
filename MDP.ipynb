{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43732b2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T20:36:58.024328Z",
     "start_time": "2023-12-06T20:36:58.001116300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import nessary libraries\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.cliffwalking import CliffWalkingEnv\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "from os import path\n",
    "from numpy import argmax\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a570958c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T20:36:58.064063300Z",
     "start_time": "2023-12-06T20:36:58.017806100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Do not change this class\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "image_path = path.join(path.dirname(gym.__file__), \"envs\", \"toy_text\")\n",
    "\n",
    "class CliffWalking(CliffWalkingEnv):\n",
    "    def __init__(self, is_hardmode=True, num_cliffs=10, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.is_hardmode = is_hardmode\n",
    "\n",
    "        # Generate random cliff positions\n",
    "        if self.is_hardmode:\n",
    "            self.num_cliffs = num_cliffs\n",
    "            self._cliff = np.zeros(self.shape, dtype=bool)\n",
    "            self.start_state = (3, 0)\n",
    "            self.terminal_state = (self.shape[0] - 1, self.shape[1] - 1)\n",
    "            self.cliff_positions = []\n",
    "            while len(self.cliff_positions) < self.num_cliffs:\n",
    "                new_row = np.random.randint(0, 4)\n",
    "                new_col = np.random.randint(0, 11)\n",
    "                state = (new_row, new_col)\n",
    "                if (\n",
    "                    (state not in self.cliff_positions)\n",
    "                    and (state != self.start_state)\n",
    "                    and (state != self.terminal_state)\n",
    "                ):\n",
    "                    self._cliff[new_row, new_col] = True\n",
    "                    if not self.is_valid():\n",
    "                        self._cliff[new_row, new_col] = False\n",
    "                        continue\n",
    "                    self.cliff_positions.append(state)\n",
    "\n",
    "        # Calculate transition probabilities and rewards\n",
    "        self.P = {}\n",
    "        for s in range(self.nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            self.P[s] = {a: [] for a in range(self.nA)}\n",
    "            self.P[s][UP] = self._calculate_transition_prob(position, [-1, 0])\n",
    "            self.P[s][RIGHT] = self._calculate_transition_prob(position, [0, 1])\n",
    "            self.P[s][DOWN] = self._calculate_transition_prob(position, [1, 0])\n",
    "            self.P[s][LEFT] = self._calculate_transition_prob(position, [0, -1])\n",
    "\n",
    "    def _calculate_transition_prob(self, current, delta):\n",
    "        new_position = np.array(current) + np.array(delta)\n",
    "        new_position = self._limit_coordinates(new_position).astype(int)\n",
    "        new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n",
    "        if self._cliff[tuple(new_position)]:\n",
    "            return [(1.0, self.start_state_index, -100, False)]\n",
    "\n",
    "        terminal_state = (self.shape[0] - 1, self.shape[1] - 1)\n",
    "        is_terminated = tuple(new_position) == terminal_state\n",
    "        return [(1 / 3, new_state, -1, is_terminated)]\n",
    "\n",
    "    # DFS to check that it's a valid path.\n",
    "    def is_valid(self):\n",
    "        frontier, discovered = [], set()\n",
    "        frontier.append((3, 0))\n",
    "        while frontier:\n",
    "            r, c = frontier.pop()\n",
    "            if not (r, c) in discovered:\n",
    "                discovered.add((r, c))\n",
    "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "                for x, y in directions:\n",
    "                    r_new = r + x\n",
    "                    c_new = c + y\n",
    "                    if r_new < 0 or r_new >= self.shape[0] or c_new < 0 or c_new >= self.shape[1]:\n",
    "                        continue\n",
    "                    if (r_new, c_new) == self.terminal_state:\n",
    "                        return True\n",
    "                    if not self._cliff[r_new][c_new]:\n",
    "                        frontier.append((r_new, c_new))\n",
    "        return False\n",
    "\n",
    "    def step(self, action):\n",
    "        if action not in [0, 1, 2, 3]:\n",
    "            raise ValueError(f\"Invalid action {action}   must be in [0, 1, 2, 3]\")\n",
    "\n",
    "        if self.is_hardmode:\n",
    "            match action:\n",
    "                case 0:\n",
    "                    action = np.random.choice([0, 1, 3], p=[1 / 3, 1 / 3, 1 / 3])\n",
    "                case 1:\n",
    "                    action = np.random.choice([0, 1, 2], p=[1 / 3, 1 / 3, 1 / 3])\n",
    "                case 2:\n",
    "                    action = np.random.choice([1, 2, 3], p=[1 / 3, 1 / 3, 1 / 3])\n",
    "                case 3:\n",
    "                    action = np.random.choice([0, 2, 3], p=[1 / 3, 1 / 3, 1 / 3])\n",
    "\n",
    "        return super().step(action)\n",
    "\n",
    "    def _render_gui(self, mode):\n",
    "        try:\n",
    "            import pygame\n",
    "        except ImportError as e:\n",
    "            raise DependencyNotInstalled(\n",
    "                \"pygame is not installed, run `pip install gymnasium[toy-text]`\"\n",
    "            ) from e\n",
    "        if self.window_surface is None:\n",
    "            pygame.init()\n",
    "\n",
    "            if mode == \"human\":\n",
    "                pygame.display.init()\n",
    "                pygame.display.set_caption(\"CliffWalking - Edited by Audrina & Kian\")\n",
    "                self.window_surface = pygame.display.set_mode(self.window_size)\n",
    "            else:  # rgb_array\n",
    "                self.window_surface = pygame.Surface(self.window_size)\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "        if self.elf_images is None:\n",
    "            hikers = [\n",
    "                path.join(image_path, \"img/elf_up.png\"),\n",
    "                path.join(image_path, \"img/elf_right.png\"),\n",
    "                path.join(image_path, \"img/elf_down.png\"),\n",
    "                path.join(image_path, \"img/elf_left.png\"),\n",
    "            ]\n",
    "            self.elf_images = [\n",
    "                pygame.transform.scale(pygame.image.load(f_name), self.cell_size)\n",
    "                for f_name in hikers\n",
    "            ]\n",
    "        if self.start_img is None:\n",
    "            file_name = path.join(image_path, \"img/stool.png\")\n",
    "            self.start_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "        if self.goal_img is None:\n",
    "            file_name = path.join(image_path, \"img/cookie.png\")\n",
    "            self.goal_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "        if self.mountain_bg_img is None:\n",
    "            bg_imgs = [\n",
    "                path.join(image_path, \"img/mountain_bg1.png\"),\n",
    "                path.join(image_path, \"img/mountain_bg2.png\"),\n",
    "            ]\n",
    "            self.mountain_bg_img = [\n",
    "                pygame.transform.scale(pygame.image.load(f_name), self.cell_size)\n",
    "                for f_name in bg_imgs\n",
    "            ]\n",
    "        if self.near_cliff_img is None:\n",
    "            near_cliff_imgs = [\n",
    "                path.join(image_path, \"img/mountain_near-cliff1.png\"),\n",
    "                path.join(image_path, \"img/mountain_near-cliff2.png\"),\n",
    "            ]\n",
    "            self.near_cliff_img = [\n",
    "                pygame.transform.scale(pygame.image.load(f_name), self.cell_size)\n",
    "                for f_name in near_cliff_imgs\n",
    "            ]\n",
    "        if self.cliff_img is None:\n",
    "            file_name = path.join(image_path, \"img/mountain_cliff.png\")\n",
    "            self.cliff_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "\n",
    "        for s in range(self.nS):\n",
    "            row, col = np.unravel_index(s, self.shape)\n",
    "            pos = (col * self.cell_size[0], row * self.cell_size[1])\n",
    "            check_board_mask = row % 2 ^ col % 2\n",
    "            self.window_surface.blit(self.mountain_bg_img[check_board_mask], pos)\n",
    "\n",
    "            if self._cliff[row, col]:\n",
    "                self.window_surface.blit(self.cliff_img, pos)\n",
    "            if s == self.start_state_index:\n",
    "                self.window_surface.blit(self.start_img, pos)\n",
    "            if s == self.nS - 1:\n",
    "                self.window_surface.blit(self.goal_img, pos)\n",
    "            if s == self.s:\n",
    "                elf_pos = (pos[0], pos[1] - 0.1 * self.cell_size[1])\n",
    "                last_action = self.lastaction if self.lastaction is not None else 2\n",
    "                self.window_surface.blit(self.elf_images[last_action], elf_pos)\n",
    "\n",
    "        if mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.window_surface)), axes=(1, 0, 2)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e3bfe55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T20:36:58.331200100Z",
     "start_time": "2023-12-06T20:36:58.051082400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create an environment\n",
    "env = CliffWalking(render_mode=\"human\")\n",
    "observation, info = env.reset(seed=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def policy_iteration(policy, states, actions, P, gamma, theta=0.001):\n",
    "    # Initialize random policy\n",
    "    # policy = {state: random.choice(actions) for state in states}\n",
    "    while True:\n",
    "        # Policy evaluation\n",
    "        V = {state: 0 for state in states}\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for state in states:\n",
    "                v = V[state]\n",
    "                action = policy[state]\n",
    "                value = 0\n",
    "                for next_state in states:\n",
    "                    info = P[state][action][0]\n",
    "                    if info[1] == next_state:\n",
    "                        if info[3] is True:\n",
    "                            value += info[0] * (100 + gamma * V[next_state])\n",
    "                        else:\n",
    "                            value += info[0] * (info[2] + gamma * V[next_state])\n",
    "                V[state] = value                \n",
    "                delta = max(delta, abs(v - V[state]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        # Policy improvement\n",
    "        policy_stable = True\n",
    "        for state in states:\n",
    "            old_action = policy[state]\n",
    "            max_value = float('-inf')\n",
    "            best_action = None\n",
    "            for action in actions:\n",
    "                action_value = 0\n",
    "                for next_state in states:\n",
    "                    info = P[state][action][0]\n",
    "                    if info[1] == next_state:\n",
    "                        if info[3] is True:\n",
    "                            action_value += info[0] * (100 + gamma * V[next_state])\n",
    "                        else:\n",
    "                            action_value += info[0] * (info[2] + gamma * V[next_state])\n",
    "                if action_value > max_value:\n",
    "                    max_value = action_value\n",
    "                    best_action = action\n",
    "            policy[state] = best_action\n",
    "            if old_action != best_action:\n",
    "                # policy[state] = best_action\n",
    "                policy_stable = False\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return policy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T20:36:58.344103400Z",
     "start_time": "2023-12-06T20:36:58.337911700Z"
    }
   },
   "id": "eb9e7a97fda7c5b6"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# def policy_evaluation(policy, states, P, gamma, theta=0.001):\n",
    "#     V = {state: 0 for state in states}\n",
    "#     while True:\n",
    "#         delta = 0\n",
    "#         for state in states:\n",
    "#             v = V[state]\n",
    "#             action = policy[state]\n",
    "#             value = 0\n",
    "#             info = P[state][action][0]\n",
    "#             for next_state in states:\n",
    "#                 if info[1] == next_state:\n",
    "#                     if info[3] is True:\n",
    "#                         value += info[0] * (100 + gamma * V[next_state])\n",
    "#                     else:\n",
    "#                         value += info[0] * (info[2] + gamma * V[next_state])\n",
    "#             V[state] = value\n",
    "#             delta = max(delta, abs(v - V[state]))\n",
    "#         if delta < theta:\n",
    "#             return V"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T20:36:58.357548800Z",
     "start_time": "2023-12-06T20:36:58.347029200Z"
    }
   },
   "id": "a4a81e055c75e581"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e0bed6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T21:02:08.339377600Z",
     "start_time": "2023-12-06T21:01:50.443839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0, 2: 2, 3: 2, 4: 1, 5: 2, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 2, 12: 0, 13: 0, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 2, 20: 2, 21: 2, 22: 1, 23: 2, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1, 34: 1, 35: 2, 36: 0, 37: 1, 38: 1, 39: 1, 40: 0, 41: 1, 42: 1, 43: 0, 44: 0, 45: 1, 46: 1, 47: 1}\n",
      "**************\n",
      "{0: 0, 1: 0, 2: 2, 3: 2, 4: 1, 5: 2, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 2, 12: 0, 13: 0, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 2, 20: 2, 21: 2, 22: 1, 23: 2, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1, 34: 1, 35: 2, 36: 0, 37: 1, 38: 1, 39: 1, 40: 0, 41: 1, 42: 1, 43: 0, 44: 0, 45: 1, 46: 1, 47: 1}\n",
      "**************\n",
      "{0: 0, 1: 0, 2: 2, 3: 2, 4: 1, 5: 2, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 2, 12: 0, 13: 0, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 2, 20: 2, 21: 2, 22: 1, 23: 2, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1, 34: 1, 35: 2, 36: 0, 37: 1, 38: 1, 39: 1, 40: 0, 41: 1, 42: 1, 43: 0, 44: 0, 45: 1, 46: 1, 47: 1}\n",
      "**************\n",
      "{0: 0, 1: 0, 2: 2, 3: 2, 4: 1, 5: 2, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 2, 12: 0, 13: 0, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 2, 20: 2, 21: 2, 22: 1, 23: 2, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1, 34: 1, 35: 2, 36: 0, 37: 1, 38: 1, 39: 1, 40: 0, 41: 1, 42: 1, 43: 0, 44: 0, 45: 1, 46: 1, 47: 1}\n",
      "**************\n",
      "{0: 0, 1: 0, 2: 2, 3: 2, 4: 1, 5: 2, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 2, 12: 0, 13: 0, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 2, 20: 2, 21: 2, 22: 1, 23: 2, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1, 34: 1, 35: 2, 36: 0, 37: 1, 38: 1, 39: 1, 40: 0, 41: 1, 42: 1, 43: 0, 44: 0, 45: 1, 46: 1, 47: 1}\n",
      "**************\n",
      "{0: 0, 1: 0, 2: 2, 3: 2, 4: 1, 5: 2, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 2, 12: 0, 13: 0, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 2, 20: 2, 21: 2, 22: 1, 23: 2, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1, 34: 1, 35: 2, 36: 0, 37: 1, 38: 1, 39: 1, 40: 0, 41: 1, 42: 1, 43: 0, 44: 0, 45: 1, 46: 1, 47: 1}\n",
      "**************\n",
      "{0: 0, 1: 0, 2: 2, 3: 2, 4: 1, 5: 2, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 2, 12: 0, 13: 0, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 2, 20: 2, 21: 2, 22: 1, 23: 2, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1, 34: 1, 35: 2, 36: 0, 37: 1, 38: 1, 39: 1, 40: 0, 41: 1, 42: 1, 43: 0, 44: 0, 45: 1, 46: 1, 47: 1}\n",
      "**************\n",
      "{0: 0, 1: 0, 2: 2, 3: 2, 4: 1, 5: 2, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 2, 12: 0, 13: 0, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 2, 20: 2, 21: 2, 22: 1, 23: 2, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1, 34: 1, 35: 2, 36: 0, 37: 1, 38: 1, 39: 1, 40: 0, 41: 1, 42: 1, 43: 0, 44: 0, 45: 1, 46: 1, 47: 1}\n",
      "**************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[41], line 45\u001B[0m\n\u001B[0;32m     43\u001B[0m s \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m36\u001B[39m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m---> 45\u001B[0m     next_state, reward, done, truncated, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(policy[s])\n\u001B[0;32m     46\u001B[0m     s \u001B[38;5;241m=\u001B[39m next_state\n\u001B[0;32m     47\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m reward \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m100\u001B[39m:\n",
      "Cell \u001B[1;32mIn[33], line 91\u001B[0m, in \u001B[0;36mCliffWalking.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_hardmode:\n\u001B[0;32m     81\u001B[0m     \u001B[38;5;28;01mmatch\u001B[39;00m action:\n\u001B[0;32m     82\u001B[0m         \u001B[38;5;28;01mcase\u001B[39;00m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     83\u001B[0m             action \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mchoice([\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m], p\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m3\u001B[39m])\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     88\u001B[0m         \u001B[38;5;28;01mcase\u001B[39;00m \u001B[38;5;241m3\u001B[39m:\n\u001B[0;32m     89\u001B[0m             action \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mchoice([\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m], p\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m3\u001B[39m])\n\u001B[1;32m---> 91\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\cliffwalking.py:181\u001B[0m, in \u001B[0;36mCliffWalkingEnv.step\u001B[1;34m(self, a)\u001B[0m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlastaction \u001B[38;5;241m=\u001B[39m a\n\u001B[0;32m    180\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 181\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender()\n\u001B[0;32m    182\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28mint\u001B[39m(s), r, t, \u001B[38;5;28;01mFalse\u001B[39;00m, {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprob\u001B[39m\u001B[38;5;124m\"\u001B[39m: p})\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\cliffwalking.py:206\u001B[0m, in \u001B[0;36mCliffWalkingEnv.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    204\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_render_text()\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 206\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_render_gui(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode)\n",
      "Cell \u001B[1;32mIn[33], line 176\u001B[0m, in \u001B[0;36mCliffWalking._render_gui\u001B[1;34m(self, mode)\u001B[0m\n\u001B[0;32m    174\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mevent\u001B[38;5;241m.\u001B[39mpump()\n\u001B[0;32m    175\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mdisplay\u001B[38;5;241m.\u001B[39mupdate()\n\u001B[1;32m--> 176\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclock\u001B[38;5;241m.\u001B[39mtick(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrender_fps\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# rgb_array\u001B[39;00m\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mtranspose(\n\u001B[0;32m    179\u001B[0m         np\u001B[38;5;241m.\u001B[39marray(pygame\u001B[38;5;241m.\u001B[39msurfarray\u001B[38;5;241m.\u001B[39mpixels3d(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwindow_surface)), axes\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    180\u001B[0m     )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Define the maximum number of iterations\n",
    "max_iter_number = 10\n",
    "states = env.P.keys()\n",
    "actions = list(range(4))\n",
    "# gamma = 0.9\n",
    "policy = {state: random.choice(actions) for state in states}\n",
    "# V = {state: 0 for state in states}\n",
    "\n",
    "for __ in range(max_iter_number):\n",
    "    # TODO: Implement the agent policy here\n",
    "    # Note: .sample() is used to sample random action from the environment's action space\n",
    "    policy = policy_iteration(policy, states, actions, env.P, 0.9) \n",
    "    print(policy)\n",
    "    print(\"**************\")\n",
    "    # V = policy_evaluation(policy, states, env.P, gamma)\n",
    "    # for state in states:\n",
    "    #     old_action = policy[state]\n",
    "    #     max_value = float('-inf')\n",
    "    #     best_action = None\n",
    "    #     if old_action == 0:\n",
    "    #         actions = [0,1,3]\n",
    "    #     if old_action == 1:\n",
    "    #         actions = [0,1,2]\n",
    "    #     if old_action == 2:\n",
    "    #         actions = [1,2,3]\n",
    "    #     if old_action == 3:\n",
    "    #         actions = [0,2,3]\n",
    "    #     for action in actions:\n",
    "    #         action_value = 0\n",
    "    #         info = env.P[state][action][0]\n",
    "    #         for next_state in states:\n",
    "    #             if info[1] == next_state:\n",
    "    #                 if info[3] is True:\n",
    "    #                     action_value += info[0] * (100 + gamma * V[next_state])\n",
    "    #                 else:\n",
    "    #                     action_value += info[0] * (info[2] + gamma * V[next_state])\n",
    "    #         if action_value > max_value:\n",
    "    #             max_value = action_value\n",
    "    #             best_action = action\n",
    "    #     # policy[state] = best_action\n",
    "    #     if old_action != best_action:\n",
    "    #         policy[state] = best_action\n",
    "    s = 36\n",
    "    while True:\n",
    "        next_state, reward, done, truncated, info = env.step(policy[s])\n",
    "        s = next_state\n",
    "        if reward == -100:\n",
    "            break\n",
    "        if done or truncated:\n",
    "            observation, info = env.reset()\n",
    "    # Choose an action (Replace this random action with your agent's policy)\n",
    "    # action = policy\n",
    "\n",
    "    # Perform the action and receive feedback from the environment\n",
    "    # next_state, reward, done, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f712696",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T20:38:07.034603Z",
     "start_time": "2023-12-06T20:38:07.026517Z"
    }
   },
   "outputs": [],
   "source": [
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-06T20:38:07.029516700Z"
    }
   },
   "id": "47b877d909c27f8b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
